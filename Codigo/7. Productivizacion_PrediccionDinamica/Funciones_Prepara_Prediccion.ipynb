{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a771d4d",
   "metadata": {},
   "source": [
    "## Funciones_Prepara_Prediccion.ipynb\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "Declaración y desarrollo de funciones utilizadas en el proceso de predicción por estación para los días futuros.\n",
    "\n",
    "### Descripción General de notebook\n",
    "\n",
    "    Funciones:\n",
    "    1. _baseMeteorologia\n",
    "    2. _baseFestivos\n",
    "    3. _prediccionMeteorologia\n",
    "    4. _esFestivo\n",
    "    5. _estacionEstado\n",
    "    6. _estacionDataFechaNueva\n",
    "    7. _dataBaseOriginal\n",
    "    8. _dataBaseOriginalNula\n",
    "    9. date_range\n",
    "    10. _lecturaDatosOrigen\n",
    "    11. _PredicionesRanking\n",
    "    12. _EjecutaPrediccion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb5d10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, requests\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import io\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../1. Librerias Mongo')\n",
    "\n",
    "from MongoDB_Connections import _connect_mongo\n",
    "from MongoDB_Funciones_Consultas import _data_anio_mes\n",
    "from MongoDB_Funciones_Consultas import _data_anio\n",
    "from MongoDB_Funciones_Consultas import _data_anio_mes_por_estacion\n",
    "from MongoDB_Funciones_Consultas import _data_anio_por_estacion\n",
    "from MongoDB_Funciones_Consultas import _consulta_meteoUS_por_anio\n",
    "from MongoDB_Funciones_Consultas import _consulta_meteoUS_por_anio_mes\n",
    "from MongoDB_Funciones_Consultas import _consulta_meteoUS_por_anio_mes_dia\n",
    "from MongoDB_Funciones_Consultas import _consulta_stations\n",
    "from MongoDB_Funciones_Consultas import _consulta_stations_EstacionesMeteo\n",
    "from MongoDB_Funciones_Consultas import _consulta_Demografia\n",
    "from MongoDB_Funciones_Consultas import _consulta_EsFestivo\n",
    "\n",
    "# %run \"../1. Librerias Mongo/MongoDB_Funciones_Consultas.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bef83",
   "metadata": {},
   "source": [
    "### 1. _baseMeteorologia\n",
    "\n",
    "Consulta y formateo de BD de meteorología existente en colección en MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "931a91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _baseMeteorologia(db_conn):   \n",
    "\n",
    "    data_Meteo = _consulta_meteoUS_full(db_conn)\n",
    "\n",
    "    # Se agrega campo FECHA para join\n",
    "    data_Meteo['FECHA'] =pd.to_datetime({'year': data_Meteo['ANIO'],\n",
    "                                              'month': data_Meteo[\"MES\"],\n",
    "                                              'day':  data_Meteo['DIA']},\n",
    "                                              format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    data_Meteo['TEMPERATURA'] = pd.to_numeric(data_Meteo['TEMPERATURA'])\n",
    "    data_Meteo['VIENTO'] = pd.to_numeric(data_Meteo['VIENTO'])\n",
    "    data_Meteo['PRESION'] = pd.to_numeric(data_Meteo['PRESION'])\n",
    "    data_Meteo['HUMEDAD'] = pd.to_numeric(data_Meteo['HUMEDAD'])\n",
    "    data_Meteo['PRECIPITACION_1h'] = pd.to_numeric(data_Meteo['PRECIPITACION_1h'])\n",
    "    data_Meteo['PRECIPITACION_3h'] = pd.to_numeric(data_Meteo['PRECIPITACION_3h'])\n",
    "\n",
    "    data_Meteo = data_Meteo.groupby(['ANIO', 'MES', 'DIA', 'FECHA']).agg(TEMP_MAX= ('TEMPERATURA','max'),\n",
    "                                     TEMP_MIN = ('TEMPERATURA','min'),\n",
    "                                     HUMEDAD = ('HUMEDAD','mean'),\n",
    "                                     VIENTO = ('VIENTO','mean'),\n",
    "                                     PRESION = ('PRESION', 'mean'),\n",
    "                                     PRECIPITACION_1h = ('PRECIPITACION_1h', 'sum'),\n",
    "                                     PRECIPITACION_3h = ('PRECIPITACION_3h', 'sum'),\n",
    "                                     DESC_TIEMPO = ('DESC_TIEMPO', lambda x: x.value_counts().index[0]))\n",
    "\n",
    "    data_Meteo.reset_index(inplace=True)\n",
    "    \n",
    "    return data_Meteo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5431df1",
   "metadata": {},
   "source": [
    "### 2. _baseFestivos\n",
    "\n",
    "Consulta y formateo de BD de días festivos existente en colección en MongoDB Atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de5396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FECHAS DE FESTIVOS\n",
    "def _baseFestivos():\n",
    "    df_festivos = pd.read_csv('../../data/Festivos_Madrid.csv', sep=';')\n",
    "    # Se agrega campo FECHA para join\n",
    "    df_festivos['FECHA'] =pd.to_datetime({'year': df_festivos['ANIO'],\n",
    "                                              'month': df_festivos[\"MES\"]\n",
    "                                              ,'day':  df_festivos['DIA']},\n",
    "                                              format='%d-%m-%Y', errors='coerce')\n",
    "    return df_festivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93204db7",
   "metadata": {},
   "source": [
    "### 3. _prediccionMeteorologia\n",
    "\n",
    "Retorno de datos meteorológicos según son consultados vía API a los servicios web de OpenWeather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7122c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recibe:\n",
    "# fecha: fecha a consultar en formato datetime.datetime\n",
    "# db_conn: conexión a Mongo, colección Meteo_US_NivelHora\n",
    "def _prediccionMeteorologia(fecha, db_conn):\n",
    "    \n",
    "    # Los datos meteos se buscarán en OpenWeather si la fecha actual está en los siguientes 14 dias a partir de la fecha actual\n",
    "    # Si la fecha es anterior a la actual los datos se buscarán en Mongo\n",
    "    \n",
    "    fechaIniQuincena = datetime.datetime.today().date() # Fecha de hoy\n",
    "    fechaTerQuincena = fechaIniQuincena + datetime.timedelta(days=14) # 14 dias a partir de la fecha de hoy\n",
    "    fecha_str = fecha.strftime('%d/%m/%Y')\n",
    "    \n",
    "    if ((fecha>=fechaIniQuincena) & (fecha<=fechaTerQuincena)):    # OpenWeather\n",
    "    \n",
    "        lat='40.41831'                                     # Latitude Madrid Center\n",
    "        lon='-3.70275'                                     # Longitute Madrid Center\n",
    "        units='metric'                                     # Temp: °C, Dist: meters\n",
    "        lang='en'                                          # Weather description in english\n",
    "        appid = APPID                                      # Secret token\n",
    "        cnt = 16                                           # Cantidad de días que desea predecir\n",
    "        req=f'https://api.openweathermap.org/data/2.5/forecast/daily?lat={lat}&lon={lon}&units={units}&lang={lang}&appid={appid}&cnt={cnt}'\n",
    "        response = requests.get(req)\n",
    "        # Convertir los datos json recibidos en un DataFrame pandas\n",
    "\n",
    "        text = response.text\n",
    "        weather= json.loads(text)\n",
    "        weather_df_temp = pd.json_normalize(weather, record_path =['list'])\n",
    "        \n",
    "        # \"weather\" es un json anidado\n",
    "        weather_df_temp2 = pd.json_normalize(weather['list'], record_path =['weather'])\n",
    "        weather_df = pd.DataFrame()\n",
    "        # Seleccionar y formatar los campos necesarios para el modelo\n",
    "        weather_df['dt']= weather_df_temp['dt'].apply(datetime.datetime.fromtimestamp)\n",
    "        weather_df['FECHA'] = weather_df['dt'].dt.strftime('%d/%m/%Y')\n",
    "        weather_df[['TEMP_MAX','TEMP_MIN','HUMEDAD','VIENTO', 'PRESION']] = \\\n",
    "            weather_df_temp[['temp.max','temp.min','humidity','speed','pressure']]\n",
    "        \n",
    "        # Cuando no tiene predicción de lluvia, la columna PRECIPITACION no existe, \n",
    "        # por eso las creamos con valor 0\n",
    "\n",
    "#         if 'main.rain.1h' in weather_df_temp.columns:\n",
    "#             weather_df['PRECIPITACION_1h'] = weather_df_temp['rain']\n",
    "#         else:\n",
    "#             weather_df['PRECIPITACION_1h'] = 0\n",
    "        \n",
    "        weather_df['PRECIPITACION_1h'] = weather_df_temp['rain'].fillna(0)\n",
    "        weather_df['PRECIPITACION_3h']= weather_df['PRECIPITACION_1h']  \n",
    "        weather_df['DESC_TIEMPO'] = weather_df_temp2['main']\n",
    "        \n",
    "        weather_out = weather_df.loc[(weather_df['FECHA'] == fecha_str)].drop(['dt','FECHA'], axis=1)\n",
    "        \n",
    "    else: # data en Mongo\n",
    "        \n",
    "        df_meteo = _consulta_meteoUS_por_anio_mes_dia(db_conn, fecha.year, fecha.month, fecha.day)\n",
    "        \n",
    "        weather_out = df_meteo.groupby(['ANIO','MES','DIA']).agg(\n",
    "            TEMP_MAX= ('TEMPERATURA','max'),\n",
    "            TEMP_MIN = ('TEMPERATURA','min'),\n",
    "            HUMEDAD = ('HUMEDAD','mean'),\n",
    "            VIENTO = ('VIENTO','mean'),\n",
    "            PRESION = ('PRESION', 'mean'),\n",
    "            PRECIPITACION_1h = ('PRECIPITACION_1h', 'sum'),\n",
    "            PRECIPITACION_3h = ('PRECIPITACION_3h', 'sum'),\n",
    "            DESC_TIEMPO = ('DESC_TIEMPO', lambda x: x.value_counts().index[0]))\n",
    "        \n",
    "        weather_out['HUMEDAD'] = round(weather_out['HUMEDAD']).astype(int)\n",
    "        weather_out['VIENTO'] = round(weather_out['VIENTO']).astype(int)\n",
    "        weather_out['PRESION'] = round(weather_out['PRESION']).astype(int)\n",
    "        \n",
    "        weather_out = weather_out.reset_index()\n",
    "        weather_out = weather_out.drop(columns=['ANIO','MES','DIA'])        \n",
    "        \n",
    "    return weather_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17244d2d",
   "metadata": {},
   "source": [
    "### 4. _esFestivo\n",
    "\n",
    "Retorno 0 si el día consultado NO es festivo y 1 si es fectivo. El origen de la respuesta es una API de la Comunidad de Madrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _esFestivo(fecha):\n",
    "    # Conectar con el Portal de datos abiertos del Ayuntamiento de Madrid para obtener los festivos\n",
    "\n",
    "    url= 'https://datos.madrid.es/egob/catalogo/300082-0-calendario_laboral.csv'\n",
    "\n",
    "    s=requests.get(url).content\n",
    "    c=pd.read_csv(io.StringIO(s.decode('utf-8')),sep=\";\")\n",
    "\n",
    "    festivos = c[['Día','laborable / festivo / domingo festivo']].dropna(how='all')\n",
    "    festivos['FECHA'] = pd.to_datetime(festivos['Día'], errors='coerce',format=\"%d/%m/%Y\").dt.strftime('%d/%m/%Y')\n",
    "    festivos = festivos.loc[(festivos[\"FECHA\"] == fecha)]\n",
    "    return 1 if festivos['laborable / festivo / domingo festivo'].item()==\"Festivo\" else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566196c3",
   "metadata": {},
   "source": [
    "### 5. _estacionEstado\n",
    "\n",
    "Retorno 1 si la estación BiciMad se encuentra activa al momento de la consulta, 0 si no se encuentra operativa, caso en el cual su predicción futura será 0.\n",
    "El estado de la estación se consulta sobre la API de la AEMT del Ayuntamiento de Madrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7d838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _estacionEstado(estacion):\n",
    "    # Credenciales de login usuario\n",
    "    # Obtenidos con alta de aplicación en la plataforma MobilityLabs Madrid que pertenence a EMT\n",
    "\n",
    "    XClientId = XCLIENTID\n",
    "    passKey = PASSKEY\n",
    "    # Obtener accessToken\n",
    "\n",
    "    response = requests.get('https://openapi.emtmadrid.es/v1/mobilitylabs/user/login/', headers={'X-ClientId': XClientId, 'passKey': passKey})\n",
    "    text = response.text\n",
    "    data = json.loads(text)\n",
    "    accessToken = data['data'][0]['accessToken']\n",
    "    \n",
    "    # Hago la consulta en la API con el accessToken obtenido en el paso anterior\n",
    "\n",
    "    response2 = requests.get('https://openapi.emtmadrid.es/v1/transport/bicimad/stations/', headers={'accessToken': accessToken})\n",
    "    text = response2.text\n",
    "    data2 = json.loads(text)['data']\n",
    "    df_situation = pd.DataFrame(data2)\n",
    "        \n",
    "    if estacion not in df_situation['id'].values: # Si la estacion no existe en salida -> la estacion no existe en Bicimad\n",
    "        estado = 0    \n",
    "    else:\n",
    "        estado = 1 if df_situation.loc[(df_situation[\"id\"] == estacion)]['no_available'].item()==0 else 0\n",
    "#         df_situation.loc[(df_situation[\"id\"] == estacion)]['activate'].item()        \n",
    "    \n",
    "    return estado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8195a3",
   "metadata": {},
   "source": [
    "### 6. _estacionDataFechaNueva\n",
    "\n",
    "Construcción de DataFrame con FechaNueva que contiene los parámetros necesarios para ser entregados a la función final de predicción. Entre los parámetros necesarios:\n",
    "    - Datos meteorológicos predichos en OpenWeather\n",
    "    - EsFestivo\n",
    "    - EsFinSemana\n",
    "    otros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _estacionDataFechaNueva(idEstacion, fecha, df_Dummies, lst_CatCol, lst_NumCol, dbConnMeteo):\n",
    "    #idEstacion, fecha, data_Meteo_in, df_festivos_in, df_Dummies, lst_CatCol, lst_NumCol):\n",
    "    \n",
    "    # Variable dia semana\n",
    "    if fecha.isoweekday()==7:\n",
    "        dia_semana=1\n",
    "    else:\n",
    "        dia_semana=fecha.isoweekday()+1\n",
    "\n",
    "    # Variable Es_FinSemana\n",
    "    if ((dia_semana==1) or (dia_semana==7)):\n",
    "        es_finsemana = 1\n",
    "    else:\n",
    "        es_finsemana = 0\n",
    "\n",
    "    # Variable TEMPORADA\n",
    "    if fecha.month<=3:\n",
    "        temporada='INVIERNO'\n",
    "    elif fecha.month<=6:\n",
    "        temporada='PRIMAVERA'\n",
    "    elif fecha.month<=9:\n",
    "        temporada='VERANO'\n",
    "    else:\n",
    "        temporada='OTONO'\n",
    "    \n",
    "#     meteo = data_Meteo_in[data_Meteo_in['FECHA']==fecha.strftime('%m/%d/%Y')]\n",
    "#     es_festivo = df_festivos_in[df_festivos_in['FECHA']==fecha.strftime('%m/%d/%Y')]['FESTIVO'].iloc[0]\n",
    "\n",
    "    meteo = _prediccionMeteorologia(fecha, dbConnMeteo)\n",
    "    es_festivo = _esFestivo(fecha.strftime('%d/%m/%Y'))\n",
    "    \n",
    "    new_row_dict = {\n",
    "        'ESTACION': idEstacion,\n",
    "        'MES': fecha.month,\n",
    "        'TEMPORADA': temporada,\n",
    "        'DIA_SEMANA': dia_semana,\n",
    "        'Es_Festivo': es_festivo,\n",
    "        'Es_FinSemana': es_finsemana,\n",
    "        'TEMP_MAX': float(meteo.TEMP_MAX.iloc[0]),\n",
    "        'TEMP_MIN': float(meteo.TEMP_MIN.iloc[0]),\n",
    "        'HUMEDAD': float(meteo.HUMEDAD.iloc[0]),\n",
    "        'VIENTO': float(meteo.VIENTO.iloc[0]),\n",
    "        'PRESION': float(meteo.PRESION.iloc[0]),\n",
    "        'PRECIPITACION_1h': float(meteo.PRECIPITACION_1h.iloc[0]),\n",
    "        'PRECIPITACION_3h': float(meteo.PRECIPITACION_3h.iloc[0]),\n",
    "        'DESC_TIEMPO': meteo.DESC_TIEMPO.iloc[0]\n",
    "    }\n",
    "\n",
    "    fecha = pd.DataFrame(new_row_dict, index=[0])\n",
    "    \n",
    "    fecha['MES_sen'] = np.sin(2 * np.pi * fecha['MES'] / 11)\n",
    "    fecha['MES_cos'] = np.cos(2 * np.pi * fecha['MES'] / 11)\n",
    "    \n",
    "    fecha['DIA_SEMANA'] = fecha['DIA_SEMANA'].astype('category')\n",
    "    fecha['TEMPORADA'] = fecha['TEMPORADA'].astype('category')\n",
    "    fecha['Es_Festivo'] = fecha['Es_Festivo'].astype('category')\n",
    "    fecha['Es_FinSemana'] = fecha['Es_FinSemana'].astype('category')\n",
    "    fecha['DESC_TIEMPO'] = fecha['DESC_TIEMPO'].astype('category')\n",
    "\n",
    "\n",
    "    # Se agrega DF con valores para dummies\n",
    "    fecha = pd.concat([fecha, df_Dummies])\n",
    "    \n",
    "    # Se generan campos dummies    \n",
    "    fecha = pd.get_dummies(fecha, columns=lst_CatCol, drop_first=True)\n",
    "\n",
    "    # Standarización de variables numéricas específicas\n",
    "    df_NumCols = fecha[['TEMP_MAX','TEMP_MIN','HUMEDAD','VIENTO','PRESION','PRECIPITACION_1h','PRECIPITACION_3h']]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    fecha[['TEMP_MAX','TEMP_MIN','HUMEDAD','VIENTO','PRESION','PRECIPITACION_1h','PRECIPITACION_3h']] = scaler.fit_transform(df_NumCols)\n",
    "    \n",
    "#     fecha = fecha.drop(['MES'], axis=1)\n",
    "    \n",
    "    # Se eliminan registros usados para generar variables dummies\n",
    "    fecha = fecha.iloc[0]\n",
    "    \n",
    "    fecha = fecha[['MES_sen','MES_cos','TEMP_MAX','TEMP_MIN','HUMEDAD','VIENTO','PRESION',\n",
    "                   'PRECIPITACION_1h','PRECIPITACION_3h','Es_Festivo_1','Es_FinSemana_1',\n",
    "                   'TEMPORADA_OTONO','TEMPORADA_PRIMAVERA','TEMPORADA_VERANO', \n",
    "                   'DIA_SEMANA_2','DIA_SEMANA_3','DIA_SEMANA_4','DIA_SEMANA_5','DIA_SEMANA_6','DIA_SEMANA_7',\n",
    "                   'DESC_TIEMPO_Clouds','DESC_TIEMPO_Drizzle','DESC_TIEMPO_Fog','DESC_TIEMPO_Mist',\n",
    "                   'DESC_TIEMPO_Rain','DESC_TIEMPO_Snow','DESC_TIEMPO_Thunderstorm']]\n",
    "        \n",
    "    fecha = fecha.astype(float)\n",
    "        \n",
    "    return pd.DataFrame(fecha).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822e04a",
   "metadata": {},
   "source": [
    "### 7. _dataBaseOriginal\n",
    "\n",
    "Creación de DataFrame estandarizado con datos históricos reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f008cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar DF con datos originales\n",
    "\n",
    "def _dataBaseOriginal(filename):\n",
    "    dataBase = pd.read_csv(filename, parse_dates=['FECHA'])\n",
    "    \n",
    "    dataBiciMad = dataBase.groupby(['ESTACION','ANIO','MES','DIA','TEMPORADA','DIA_SEMANA', 'Es_Festivo', 'Es_FinSemana',\n",
    "                          ]).agg(DEMANDA=('DEMANDA', 'sum'),\n",
    "                                 TEMP_MAX= ('TEMPERATURA','max'),\n",
    "                                 TEMP_MIN = ('TEMPERATURA','min'),\n",
    "                                 HUMEDAD = ('HUMEDAD','mean'),\n",
    "                                 VIENTO = ('VIENTO','mean'),\n",
    "                                 PRESION = ('PRESION', 'mean'),\n",
    "                                 PRECIPITACION_1h = ('PRECIPITACION_1h', 'sum'),\n",
    "                                 PRECIPITACION_3h = ('PRECIPITACION_3h', 'sum'),\n",
    "                                 DESC_TIEMPO = ('DESC_TIEMPO', lambda x: x.value_counts().index[0]))\n",
    "    \n",
    "    dataBiciMad_DF = dataBiciMad.reset_index() \n",
    "    \n",
    "    dataBiciMad_DF['MES_sen'] = np.sin(2 * np.pi * dataBiciMad_DF['MES'] / 11)\n",
    "    dataBiciMad_DF['MES_cos'] = np.cos(2 * np.pi * dataBiciMad_DF['MES'] / 11)\n",
    "    \n",
    "    dataBiciMad_DF['DIA_SEMANA'] = dataBiciMad_DF['DIA_SEMANA'].astype('category')\n",
    "    dataBiciMad_DF['Es_Festivo'] = dataBiciMad_DF['Es_Festivo'].astype('category')\n",
    "    dataBiciMad_DF['Es_FinSemana'] = dataBiciMad_DF['Es_FinSemana'].astype('category')\n",
    "    dataBiciMad_DF['DESC_TIEMPO'] = dataBiciMad_DF['DESC_TIEMPO'].astype('category')\n",
    "    \n",
    "    dataBiciMad_DF['DEMANDA'] =  np.log1p(dataBiciMad_DF['DEMANDA'])\n",
    "    \n",
    "    cols = dataBiciMad_DF.columns\n",
    "    num_cols = dataBiciMad_DF._get_numeric_data().columns\n",
    "    cat_cols = list(set(cols) - set(num_cols))\n",
    "    \n",
    "    dataBiciMad_DF = pd.get_dummies(dataBiciMad_DF, columns=cat_cols, drop_first=True)\n",
    "\n",
    "    df_NumCols = dataBiciMad_DF[['TEMP_MAX','TEMP_MIN','HUMEDAD','VIENTO','PRESION','PRECIPITACION_1h','PRECIPITACION_3h']]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    dataBiciMad_DF[['TEMP_MAX','TEMP_MIN','HUMEDAD','VIENTO','PRESION','PRECIPITACION_1h','PRECIPITACION_3h']] = scaler.fit_transform(df_NumCols)\n",
    "    \n",
    "    dataBiciMad_DF = dataBiciMad_DF.drop(['ANIO','MES','DIA'], axis=1)\n",
    "    \n",
    "    dataBiciMad_DF = dataBiciMad_DF[['ESTACION','DEMANDA','MES_sen','MES_cos','TEMP_MAX','TEMP_MIN','HUMEDAD','VIENTO','PRESION',\n",
    "                   'PRECIPITACION_1h','PRECIPITACION_3h','Es_Festivo_1','Es_FinSemana_1',\n",
    "                   'TEMPORADA_OTONO','TEMPORADA_PRIMAVERA','TEMPORADA_VERANO', \n",
    "                   'DIA_SEMANA_2','DIA_SEMANA_3','DIA_SEMANA_4','DIA_SEMANA_5','DIA_SEMANA_6','DIA_SEMANA_7',\n",
    "                   'DESC_TIEMPO_Clouds','DESC_TIEMPO_Drizzle','DESC_TIEMPO_Fog','DESC_TIEMPO_Mist',\n",
    "                   'DESC_TIEMPO_Rain','DESC_TIEMPO_Snow','DESC_TIEMPO_Thunderstorm']]\n",
    "    \n",
    "    return dataBiciMad_DF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85027c9b",
   "metadata": {},
   "source": [
    "### 8. _dataBaseOriginalNula\n",
    "\n",
    "Creación de DataFrame estandarizado con demanda nula pero con valores de variables explicativas existentes en base de datos históricos reales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dd7db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar DF con datos originales pero ESTACION=0 y sin DEMANDA\n",
    "\n",
    "def _dataBaseOriginalNula(filename):\n",
    "    dataBase = pd.read_csv(filename, parse_dates=['FECHA'])\n",
    "    \n",
    "    dataBiciMad = dataBase.groupby(['ESTACION','ANIO','MES','DIA','TEMPORADA','DIA_SEMANA', 'Es_Festivo', 'Es_FinSemana',\n",
    "                          ]).agg(\n",
    "                                 TEMP_MAX= ('TEMPERATURA','max'),\n",
    "                                 TEMP_MIN = ('TEMPERATURA','min'),\n",
    "                                 HUMEDAD = ('HUMEDAD','mean'),\n",
    "                                 VIENTO = ('VIENTO','mean'),\n",
    "                                 PRESION = ('PRESION', 'mean'),\n",
    "                                 PRECIPITACION_1h = ('PRECIPITACION_1h', 'sum'),\n",
    "                                 PRECIPITACION_3h = ('PRECIPITACION_3h', 'sum'),\n",
    "                                 DESC_TIEMPO = ('DESC_TIEMPO', lambda x: x.value_counts().index[0]))\n",
    "    dataBiciMad = dataBiciMad.reset_index()    \n",
    "    dataBiciMad['ESTACION'] = 0\n",
    "    \n",
    "    return dataBiciMad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c3d7bb",
   "metadata": {},
   "source": [
    "### 9. date_range\n",
    "\n",
    "Rango de dias entre 2 fechas específicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "764ac1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_range(start, end):\n",
    "    delta = end - start  # as timedelta\n",
    "    days = [start + timedelta(days=i) for i in range(delta.days + 1)]\n",
    "    return days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a4906e",
   "metadata": {},
   "source": [
    "### 10. _lecturaDatosOrigen\n",
    "\n",
    "Lectura de archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d723b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lecturaDatosOrigen(filename):\n",
    "    datosOrigen = pd.read_csv(filename)\n",
    "    return datosOrigen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92125c15",
   "metadata": {},
   "source": [
    "### 11. _PredicionesRanking\n",
    "\n",
    "Creación de ranking para dias futuros de cluster basados en predicción de las estaciones que los componen. Se generan 4 rankings:\n",
    "    - Ranking según lista de 5 clusters\n",
    "    - Ranking según lista de 7 clusters\n",
    "    - Ranking según lista de Barrios de Madrid\n",
    "    - Ranking según lista de Distritos de Madrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5439ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objetivo: generar DF final de predicciones con ranking de cluster, barrios y distritos calculados en base a predicción predicha\n",
    "def _PredicionesRanking(prediccionesIn, datosDF):\n",
    "    \n",
    "    # DESCARGA DE CLUSTER's y BARRIO/DISTRITO DE CADA ESTACION\n",
    "    estaciones_cluster = datosDF[['ESTACION', 'CLUSTER_soloGeo5', 'CLUSTER_soloGeo7']].drop_duplicates()\n",
    "    \n",
    "    db_Estacion = _connect_mongo('cloud', 'cluster0.15npsxw.mongodb.net', None, 'ucmtfm2022', 'UCM_2022', 'BiciMAD', 'Estaciones')\n",
    "    estaciones_barrios = _consulta_stations(db_Estacion)[['Id_Estacion', 'Barrio', 'Barrio_Nombre', 'Distrito', \\\n",
    "                                                      'Distrito_Nombre']].rename(columns={'Id_Estacion':'ESTACION'})\n",
    "    \n",
    "    estaciones = pd.merge(estaciones_cluster, estaciones_barrios)\n",
    "    \n",
    "    Predicciones = pd.merge(prediccionesIn, estaciones, how='left', left_on='ESTACION', right_on='ESTACION')\n",
    "    \n",
    "    \n",
    "    # Creación de DF por tipo de Cluster: 5 Cluster, 7 Cluster y Barrio\n",
    "\n",
    "    PrediccionCluster5 = Predicciones.groupby(['DIA','CLUSTER_soloGeo5']).agg(PREDICCION=('PREDICCION','sum'), NESTACIONES=('ESTACION', 'count'))\n",
    "    PrediccionCluster5.reset_index(inplace=True)\n",
    "\n",
    "    PrediccionCluster7 = Predicciones.groupby(['DIA','CLUSTER_soloGeo7']).agg(PREDICCION=('PREDICCION','sum'), NESTACIONES=('ESTACION', 'count'))\n",
    "    PrediccionCluster7.reset_index(inplace=True)\n",
    "\n",
    "    PrediccionClusterBarrio = Predicciones.groupby(['DIA','Barrio','Barrio_Nombre']).agg(PREDICCION=('PREDICCION','sum'), NESTACIONES=('ESTACION', 'count'))\n",
    "    PrediccionClusterBarrio.reset_index(inplace=True)\n",
    "\n",
    "    PrediccionClusterDistrito = Predicciones.groupby(['DIA','Distrito','Distrito_Nombre']).agg(PREDICCION=('PREDICCION','sum'), NESTACIONES=('ESTACION', 'count'))\n",
    "    PrediccionClusterDistrito.reset_index(inplace=True)\n",
    "    \n",
    "    # Demanda total diaria\n",
    "    PrediccionTotal = Predicciones.groupby('DIA').agg(PREDICCION_TOTAL=('PREDICCION','sum'))\n",
    "    PrediccionTotal = PrediccionTotal.reset_index()\n",
    "    \n",
    "    \n",
    "    # Por cada DF de predicción por tipo de cluster se agrega la columna de RATIO_FINAL = (PREDICCION/PREDICCION_TOTAL)/NESTACIONES\n",
    "\n",
    "    PrediccionRatio5 = pd.merge(PrediccionCluster5, PrediccionTotal, how='left', left_on='DIA', right_on='DIA')\n",
    "    PrediccionRatio5['RATIO'] = PrediccionRatio5['PREDICCION'] / PrediccionRatio5['PREDICCION_TOTAL']\n",
    "    PrediccionRatio5['RATIO_FINAL'] = PrediccionRatio5['RATIO'] / PrediccionRatio5['NESTACIONES']\n",
    "\n",
    "    PrediccionRatio7 = pd.merge(PrediccionCluster7, PrediccionTotal, how='left', left_on='DIA', right_on='DIA')\n",
    "    PrediccionRatio7['RATIO'] = PrediccionRatio7['PREDICCION'] / PrediccionRatio7['PREDICCION_TOTAL']\n",
    "    PrediccionRatio7['RATIO_FINAL'] = PrediccionRatio7['RATIO'] / PrediccionRatio7['NESTACIONES']\n",
    "\n",
    "    PrediccionRatioBarrio = pd.merge(PrediccionClusterBarrio, PrediccionTotal, how='left', left_on='DIA', right_on='DIA')\n",
    "    PrediccionRatioBarrio['RATIO'] = PrediccionRatioBarrio['PREDICCION'] / PrediccionRatioBarrio['PREDICCION_TOTAL']\n",
    "    PrediccionRatioBarrio['RATIO_FINAL'] = PrediccionRatioBarrio['RATIO'] / PrediccionRatioBarrio['NESTACIONES']\n",
    "\n",
    "    PrediccionRatioDistrito = pd.merge(PrediccionClusterDistrito, PrediccionTotal, how='left', left_on='DIA', right_on='DIA')\n",
    "    PrediccionRatioDistrito['RATIO'] = PrediccionRatioDistrito['PREDICCION'] / PrediccionRatioDistrito['PREDICCION_TOTAL']\n",
    "    PrediccionRatioDistrito['RATIO_FINAL'] = PrediccionRatioDistrito['RATIO'] / PrediccionRatioDistrito['NESTACIONES']\n",
    "    \n",
    "    # Se agrega columna rank en cada DF de tipo de cluster por DIA\n",
    "\n",
    "    PrediccionRatio5['rank'] = PrediccionRatio5.groupby('DIA')['RATIO_FINAL'].rank('dense', ascending=False)\n",
    "    PrediccionRatio7['rank'] = PrediccionRatio7.groupby('DIA')['RATIO_FINAL'].rank('dense', ascending=False)\n",
    "    PrediccionRatioBarrio['rank'] = PrediccionRatioBarrio.groupby('DIA')['RATIO_FINAL'].rank('dense', ascending=False)\n",
    "    PrediccionRatioDistrito['rank'] = PrediccionRatioDistrito.groupby('DIA')['RATIO_FINAL'].rank('dense', ascending=False)\n",
    "\n",
    "    \n",
    "    # Creación de DF Final que contiene Predicciones por Estación, cluster al cual pertenece y el ranking según tipo de cluster\n",
    "\n",
    "    Predicciones = Predicciones.rename(columns={'PREDICCION':'PREDICCION_ESTACION'})\n",
    "\n",
    "    PrediccionesFinal = pd.merge(Predicciones, \n",
    "                                 PrediccionRatio5[[\n",
    "                                     'DIA','CLUSTER_soloGeo5','rank','PREDICCION','NESTACIONES',\n",
    "                                     'PREDICCION_TOTAL','RATIO','RATIO_FINAL']], how='left',\n",
    "                                   left_on=['DIA','CLUSTER_soloGeo5'], right_on=['DIA','CLUSTER_soloGeo5'])\n",
    "    PrediccionesFinal.rename(columns={\n",
    "        'rank':'RANK_CLUSTER5','PREDICCION':'PREDICCION_CLUSTER5','NESTACIONES':'NESTACIONES_CLUSTER5',\n",
    "        'PREDICCION_TOTAL':'PREDICCION_TOTAL_CLUSTER5','RATIO':'RATIO_CLUSTER5','RATIO_FINAL':'RATIO_FINAL_CLUSTER5'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    PrediccionesFinal = pd.merge(PrediccionesFinal, \n",
    "                                 PrediccionRatio7[[\n",
    "                                     'DIA','CLUSTER_soloGeo7','rank','PREDICCION','NESTACIONES',\n",
    "                                     'PREDICCION_TOTAL','RATIO','RATIO_FINAL']], how='left',\n",
    "                                   left_on=['DIA','CLUSTER_soloGeo7'], right_on=['DIA','CLUSTER_soloGeo7'])\n",
    "    PrediccionesFinal.rename(columns={\n",
    "        'rank':'RANK_CLUSTER7','PREDICCION':'PREDICCION_CLUSTER7','NESTACIONES':'NESTACIONES_CLUSTER7',\n",
    "        'PREDICCION_TOTAL':'PREDICCION_TOTAL_CLUSTER7','RATIO':'RATIO_CLUSTER7','RATIO_FINAL':'RATIO_FINAL_CLUSTER7'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    PrediccionesFinal = pd.merge(PrediccionesFinal, PrediccionRatioBarrio[[\n",
    "                                     'DIA','Barrio','rank','PREDICCION','NESTACIONES',\n",
    "                                     'PREDICCION_TOTAL','RATIO','RATIO_FINAL']], how='left',\n",
    "                                   left_on=['DIA','Barrio'], right_on=['DIA','Barrio'])\n",
    "    PrediccionesFinal.rename(columns={\n",
    "        'rank':'RANK_CLUSTER_BARRIO','PREDICCION':'PREDICCION_CLUSTER_BARRIO','NESTACIONES':'NESTACIONES_CLUSTER_BARRIO',\n",
    "        'PREDICCION_TOTAL':'PREDICCION_TOTAL_CLUSTER_BARRIO','RATIO':'RATIO_CLUSTER_BARRIO',\n",
    "        'RATIO_FINAL':'RATIO_FINAL_CLUSTER_BARRIO'}, inplace=True)\n",
    "\n",
    "    PrediccionesFinal = pd.merge(PrediccionesFinal, PrediccionRatioDistrito[[\n",
    "                                     'DIA','Distrito','rank','PREDICCION','NESTACIONES',\n",
    "                                     'PREDICCION_TOTAL','RATIO','RATIO_FINAL']], how='left',\n",
    "                                   left_on=['DIA','Distrito'], right_on=['DIA','Distrito'])\n",
    "    PrediccionesFinal.rename(columns={\n",
    "        'rank':'RANK_CLUSTER_DISTRITO','PREDICCION':'PREDICCION_CLUSTER_DISTRITO','NESTACIONES':'NESTACIONES_CLUSTER_DISTRITO',\n",
    "        'PREDICCION_TOTAL':'PREDICCION_TOTAL_CLUSTER_DISTRITO','RATIO':'RATIO_CLUSTER_DISTRITO',\n",
    "        'RATIO_FINAL':'RATIO_FINAL_CLUSTER_DISTRITO'}, inplace=True)\n",
    "    \n",
    "    return PrediccionesFinal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c3874",
   "metadata": {},
   "source": [
    "### 12. _EjecutaPrediccion\n",
    "\n",
    "Procedimiento para el cálculo de predicción diaria para cada una de las estaciones BiciMad del proyecto a partir de \"fechaStart\" y para los siguientes \"diasDelta\".\n",
    "El resultado es un DataFrame con el siguiente formato: [ESTACION, ANIO, MES, DIA, PREDICCION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros:\n",
    "#   - fechaStart: fecha inicio de predicción en formato string dd/mm/yyyy\n",
    "#   - DiasDelta: cantidad de días a predecir posterior a la fechaStart\n",
    "\n",
    "def _EjecutaPrediccion(fechaStart, diasDelta, datosOrigen):    \n",
    "    \n",
    "    # CONEXION A COLLECTION DE TIEMPO EN MONGODB\n",
    "    db_Meteo = _connect_mongo('cloud', 'cluster0.15npsxw.mongodb.net', None, 'ucmtfm2022', 'UCM_2022', 'BiciMAD', \n",
    "                          'Meteo_US_NivelHora')\n",
    "    \n",
    "    # lISTA DE ESTACIONES\n",
    "    df_estaciones = datosOrigen.ESTACION.unique()\n",
    "    df_estaciones = df_estaciones[5:15]\n",
    "    df_estaciones = np.sort(df_estaciones)\n",
    "    \n",
    "    estaciones_cluster = datosOrigen[['ESTACION', 'CLUSTER_soloGeo5', 'CLUSTER_soloGeo7']].drop_duplicates()\n",
    "        \n",
    "    \n",
    "    # GENERACION DE DF CON DATOS NULOS y COLUMNAS CATEGORICAS Y NUMERICAS\n",
    "    dataDummy = _dataBaseOriginalNula(\"../../Data/DataFrame_Final_Cierre_Cluster_2017_2019.csv\")\n",
    "    cat_cols= ['MES', 'DIA_SEMANA', 'TEMPORADA', 'Es_Festivo', 'Es_FinSemana','DESC_TIEMPO']\n",
    "    num_cols= ['TEMP_MAX','TEMP_MIN','HUMEDAD','VIENTO','PRESION']\n",
    "    \n",
    "    \n",
    "    # PREDICCION DE DEMANDA POR ESTACION y DIA\n",
    "    fecha_inicio = datetime.datetime.strptime(fechaStart, '%d/%m/%Y').date()\n",
    "    fecha_termino = fecha_inicio + datetime.timedelta(days=int(diasDelta)-1)\n",
    "\n",
    "    Fechas = pd.DataFrame()\n",
    "    Predicciones = pd.DataFrame()\n",
    "\n",
    "    t_ini = time.time()\n",
    "\n",
    "    for estacion in df_estaciones:\n",
    "\n",
    "        iEstadoEstacion = _estacionEstado(estacion)\n",
    "\n",
    "        print('ESTACION:'+str(estacion)+'; ESTADO:'+str(iEstadoEstacion))\n",
    "\n",
    "        model = joblib.load('../Modelos/Modelo_'+ str(estacion) +'.pkl')\n",
    "\n",
    "        rango_fechas = date_range(fecha_inicio, fecha_termino)\n",
    "        for fecha in rango_fechas:\n",
    "            print(fecha)\n",
    "\n",
    "            FechaAPredecir = _estacionDataFechaNueva(estacion, fecha, dataDummy, cat_cols, num_cols, db_Meteo)\n",
    "\n",
    "            if Fechas.empty:\n",
    "                Fechas = FechaAPredecir.copy()\n",
    "            else:\n",
    "                Fechas = pd.concat([Fechas, FechaAPredecir])            \n",
    "\n",
    "            # ESTACION ACTIVA\n",
    "            if (iEstadoEstacion==1):\n",
    "\n",
    "                # Prediccion según modelo entrenado para cada una de las estacion\n",
    "                prediccion = model.predict(FechaAPredecir)[0]\n",
    "                prediccion = pd.DataFrame([str(estacion), fecha.year, fecha.month, fecha.day, prediccion]).T\n",
    "                predExp = round(np.expm1(prediccion[4].item()))  # Se aplica exponencial para eliminar el valor log utilizado para entrenar\n",
    "                prediccion[4] = predExp\n",
    "\n",
    "                # Se crea Predicciones sólo en primera ejecución de los loops\n",
    "                if Predicciones.empty:                \n",
    "                    Predicciones = prediccion.copy()\n",
    "                else:\n",
    "                    Predicciones = pd.concat([Predicciones, prediccion])\n",
    "\n",
    "\n",
    "            # ESTACION INACTIVA\n",
    "            else:\n",
    "                pred_nula = {0:estacion,1:fecha.year,2:fecha.month,3:fecha.day,4:0}\n",
    "                pred_nula = pd.DataFrame(data=pred_nula, index=[0])\n",
    "\n",
    "                # Se crea Predicciones sólo en primera ejecución de los loops\n",
    "                if Predicciones.empty:\n",
    "                    print('ini_nula')\n",
    "                    Predicciones = pred_nula.copy()\n",
    "                else:\n",
    "                    Predicciones = pd.concat([Predicciones, pred_nula])\n",
    "\n",
    "    t_end = time.time()\n",
    "    print ((t_end - t_ini)/60)\n",
    "\n",
    "    Predicciones.rename(columns={0:'ESTACION', 1:'ANIO', 2:'MES', 3:'DIA', 4:'PREDICCION'}, inplace=True)\n",
    "    Predicciones['ESTACION']=Predicciones['ESTACION'].astype(int)\n",
    "    Predicciones['ANIO']=Predicciones['ANIO'].astype(int)\n",
    "    Predicciones['MES']=Predicciones['MES'].astype(int)\n",
    "    Predicciones['DIA']=Predicciones['DIA'].astype(int)\n",
    "    Predicciones['PREDICCION']=Predicciones['PREDICCION']#.astype(int)\n",
    "    \n",
    "    return Predicciones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
